{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download repository Grounding DINO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nikolay/Documents/Projects/Diploma/My repos/Pseudo_label_Grounding_DINO/.venv/bin/python3\n"
     ]
    }
   ],
   "source": [
    "!which python3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nikolay/Documents/Projects/Diploma/My repos/Pseudo_label_Grounding_DINO\n"
     ]
    }
   ],
   "source": [
    "# %cd ..\n",
    "# !git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
    "# %cd GroundingDINO\n",
    "# !pip install -e .\n",
    "# !pip install -r requirements.txt\n",
    "%cd ..\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "from groundingdino.util.inference import load_model, predict\n",
    "import os\n",
    "import supervision as sv\n",
    "import torch\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import pandas as pd\n",
    "from utils import AnnotationDF\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, extensions=['.jpeg']):\n",
    "        self.img_dir = img_dir\n",
    "        self.ext = extensions\n",
    "        self.files = []\n",
    "        self._search_files()\n",
    "        \n",
    "\n",
    "    def _search_files(self):\n",
    "        self.files = []\n",
    "        for ext in self.ext:\n",
    "            self.files.extend(glob.glob(self.img_dir + '/*' + ext))\n",
    "        self.files = sorted(self.files) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        img_name = os.path.split(img_path)[-1].split('.')[0]\n",
    "        img_source = cv2.imread(img_path)\n",
    "        img_source = cv2.cvtColor(img_source, cv2.COLOR_BGR2RGB)\n",
    "        image = torch.Tensor(img_source/255).permute(2,0,1)\n",
    "        return image, img_name  \n",
    "    \n",
    "# voc_dataset = ImageDataset(img_dir='data/VOC2007/train2007', extensions=['.jpg'])\n",
    "# voc_dataloader = DataLoader(voc_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference functoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run predictions\n",
    "def run_pseudo_labelling(model, dataloader, box_threshold, text_threshold, classes):\n",
    "    # Init annotation table\n",
    "    ann_df = AnnotationDF(classes)\n",
    "\n",
    "    for img, filename in tqdm.tqdm(dataloader):\n",
    "        boxes_all = None\n",
    "\n",
    "        for cls_prompt in classes:\n",
    "            \n",
    "            boxes, logits, phrases = predict(\n",
    "                model=model,\n",
    "                image=img.squeeze(0),\n",
    "                caption=cls_prompt,\n",
    "                box_threshold=box_threshold,\n",
    "                text_threshold=text_threshold\n",
    "            )\n",
    "            if boxes_all is None:\n",
    "                boxes_all = boxes\n",
    "                logits_all = logits\n",
    "                phrases_all = phrases\n",
    "            else:\n",
    "                phrases_all.extend(phrases)\n",
    "                logits_all = torch.concat((logits_all, logits), dim=0)\n",
    "                boxes_all = torch.concat((boxes_all, boxes), dim=0)\n",
    "        ann_df.add_annotation(filename, list(zip(boxes_all, logits_all, phrases_all)))\n",
    "\n",
    "    return ann_df\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_NAME = \"groundingdino_swinb_cogcoor.pth\"\n",
    "WEIGHTS_PATH = os.path.join(\"GroundingDINO_weights\", WEIGHTS_NAME)\n",
    "CONFIG_PATH = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', \n",
    "    'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', \n",
    "    'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "model = load_model(CONFIG_PATH, WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_path = 'datasets/VOC/images/val'\n",
    "val_save_path= 'data/VOC/val'\n",
    "\n",
    "test_img_path = 'datasets/VOC/images/test'\n",
    "test_save_path= 'data/VOC/test'\n",
    "\n",
    "train_img_path = 'datasets/VOC/images/train'\n",
    "train_save_path= 'data/VOC/train'\n",
    "\n",
    "\n",
    "###########\n",
    "save_ann_path = test_save_path\n",
    "voc_dataset = ImageDataset(img_dir=test_img_path, extensions=['.jpg'])\n",
    "voc_dataset = torch.utils.data.Subset(voc_dataset,[x for x in range(100)])\n",
    "\n",
    "\n",
    "voc_dataloader = DataLoader(voc_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "100%|██████████| 100/100 [03:51<00:00,  2.32s/it]\n"
     ]
    }
   ],
   "source": [
    "### Run predictions\n",
    "ann_df = run_pseudo_labelling(model=model, dataloader=voc_dataloader, box_threshold=0.1, text_threshold=0.1, classes=classes)\n",
    "ann_df.save_annotations(save_ann_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
